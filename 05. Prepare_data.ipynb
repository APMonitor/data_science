{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data\n",
    "\n",
    "[Data Science Playlist on YouTube](https://www.youtube.com/watch?v=tBfGYKITno8&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy)\n",
    "[![Python Data Science](https://apmonitor.com/che263/uploads/Begin_Python/DataScience05.png)](https://www.youtube.com/watch?v=tBfGYKITno8&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy \"Python Data Science\")\n",
    "\n",
    "Much of data science and machine learning work is getting clean data into the correct form. This may include data cleansing to remove outliers or bad information, scaling for machine learning algorithms, splitting into train and test sets, and enumeration of string data. All of this needs to happen before regression, classification, or other model training. Fortunately, there are functions that help with automating data preparation.\n",
    "\n",
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "### Generate Sample Data\n",
    "\n",
    "Run the following cell to generate the sample data that is corrupted with NaN (not a number) and outliers that are corrupted data points far outside of the expected trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1)\n",
    "n = 100\n",
    "tt = np.linspace(0,n-1,n)\n",
    "x = np.random.rand(n)+10+np.sqrt(tt)\n",
    "y = np.random.normal(10,x*0.01,n)\n",
    "x[1] = np.nan; y[2] = np.nan  # 2 NaN (not a number)\n",
    "for i in range(3):            # add 3 outliers (bad data)\n",
    "    ri = np.random.randint(0,n)\n",
    "    x[ri] += np.random.rand()*100\n",
    "data = pd.DataFrame(np.vstack((tt,x,y)).T,\\\n",
    "                    columns=['time','x','y'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![analyze](https://apmonitor.com/che263/uploads/Begin_Python/analyze.png)\n",
    "\n",
    "### Visualize Data\n",
    "\n",
    "The outliers are shown on a semi-logy plot. The `NaN` values do not show on the plot and are missing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogy(tt,x,'r.',label='x')\n",
    "plt.semilogy(tt,y,'b.',label='y')\n",
    "plt.legend(); plt.xlabel('time')\n",
    "plt.text(50,60,'Outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "### Remove Outliers and Bad Data\n",
    "\n",
    "NaN values are removed with `numpy` by identifying rows `ix` that contain `NaN`. Next, the rows are removed with `z=z[~iz]` where `~` is a bitwise `not` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[      1,      2],\n",
    "              [ np.nan,      3],\n",
    "              [      4, np.nan],\n",
    "              [      5,      6]])\n",
    "iz = np.any(np.isnan(z), axis=1)\n",
    "print(~iz)\n",
    "z = z[~iz]\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The method `dropna` is a command to drop `NaN` rows in a `pandas` `DataFrame`. Rows 1 and 2 are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any row with bad (NaN) values\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several graphical techniques to help detect outliers. A box or histogram plot shows the 3 outlying points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(data['x'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Grubbs test or [other statistical measures](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba) can detect outliers. The Grubbs test, in particular, assumes univariate, normally distributed data and is intended to detect only a single outlier. In practice, many outliers be eliminated by removing points that violate a change limit or upper / lower bounds. The statement `data[data['x']<30]` keeps the rows where x is less than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['x']<30]\n",
    "plt.boxplot(data['x'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![expert](https://apmonitor.com/che263/uploads/Begin_Python/expert.png)\n",
    "\n",
    "### Time Activity\n",
    "\n",
    "Without looking at a clock, run this cell to record 1 second intervals for 10 seconds. When you run the cell, press `Enter` everytime you think 1 second has passed. After you collect the data, use a boxplot to identify any data points in `tsec` that are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "tsec = []\n",
    "input('Press \"Enter\" to record 1 second intervals'); t = time.time()\n",
    "for i in range(10):\n",
    "    clear_output(); input('Press \"Enter\": ' + str(i+1))\n",
    "    tsec.append(time.time()-t); t = time.time()\n",
    "clear_output(); print('Completed. Add boxplot to identify outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a boxplot to identify outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "### Scale Data\n",
    "\n",
    "The `sklearn` package has a `preprocessing` module to implement common scaling methods. The `StandardScalar` is shown below where each column is normalized to zero mean and a standard deviation of one. The common scaling methods `fit_transform(X)` to fit and transform, `transform(X)` to transform based on another fit, and `inverse_transform(Xs)` to scale back to the original representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "ds = s.fit_transform(data)\n",
    "print(ds[0:5]) # print 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value `ds` is returned as a `numpy` array so we need to convert it back to a `pandas` `DataFrame`, re-using the column names from `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(ds,columns=data.columns)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "### Divide Data\n",
    "\n",
    "Data is divided into train and test sets to separate a fraction of the rows for evaluating classification or regression models. A typical split is 80% for training and 20% for testing, although the range depends on how much data is available and the objective of the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide = int(len(ds)*0.8)\n",
    "train = ds[0:divide]\n",
    "test = ds[divide:]\n",
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` is a function in `sklearn` for the specific purpose of splitting data into train and test sets. There are options such as `shuffle=True` to randomize the selection in each set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(ds, test_size=0.2, shuffle=True)\n",
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCLab Activity\n",
    "\n",
    "![expert](https://apmonitor.com/che263/uploads/Begin_Python/expert.png)\n",
    "\n",
    "### Data with Bad Values & Outliers\n",
    "\n",
    "Generate a new data file with some randomly inserted bad data (3 minutes) or read the data file from [an online link](https://apmonitor.com/do/uploads/Main/tclab_bad_data.txt) with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tclab, time, csv\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    with tclab.TCLab() as lab:\n",
    "        with open('05-tclab.csv',mode='w',newline='') as f:\n",
    "            cw = csv.writer(f)\n",
    "            cw.writerow(['Time','Q1','Q2','T1','T2'])\n",
    "            print('t Q1 Q2 T1    T2')\n",
    "            for t in range(180):\n",
    "                T1 = lab.T1; T2 = lab.T2\n",
    "                # insert bad values\n",
    "                bad = np.random.randint(0,30)\n",
    "                T1=np.nan if bad==10 else T1\n",
    "                T2=np.nan if bad==15 else T2\n",
    "                # insert random number (possibly outlier)\n",
    "                outlier = np.random.randint(-40,150)\n",
    "                T1=outlier if bad==20 else T1\n",
    "                T2=outlier if bad==25 else T2\n",
    "                # change heater\n",
    "                if t%30==0:\n",
    "                    Q1 = np.random.randint(0,81)\n",
    "                    Q2 = np.random.randint(0,81)\n",
    "                    lab.Q1(Q1); lab.Q2(Q2)\n",
    "                cw.writerow([t,Q1,Q2,T1,T2])\n",
    "                if t%10==0:\n",
    "                    print(t,Q1,Q2,T1,T2)\n",
    "                time.sleep(1)\n",
    "            data5=pd.read_csv('05-tclab.csv')\n",
    "except:\n",
    "    print('Connect TCLab to generate new data')\n",
    "    print('Importing data from online source')\n",
    "    url = 'http://apmonitor.com/do/uploads/Main/tclab_bad_data.txt'\n",
    "    data5=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse, Scale, and Divide Data\n",
    "\n",
    "After generating and importing `data5` above, remove any rows with `NaN` values or outliers in the `T1` or `T2` columns. Scale the data with a `StandardScalar` in `scikit`. Divide the data into train (80%) and test (20%) sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
