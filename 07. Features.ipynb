{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Features\n",
    "\n",
    "[Data Science Playlist on YouTube](https://www.youtube.com/watch?v=aNVkTrCS6lE&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy)\n",
    "[![Python Data Science](https://apmonitor.com/che263/uploads/Begin_Python/DataScience07.png)](https://www.youtube.com/watch?v=aNVkTrCS6lE&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy \"Python Data Science\")\n",
    "\n",
    "**Classification** predicts *discrete labels (outcomes)* such as `yes`/`no`, `True`/`False`, or any number of discrete levels such as a letter from text recognition. An example of classification is to suggest a movie you will want to watch next (label) based on your prior viewing history (feature). **Regression** is different than classification with continuous outcomes such as any floating point number in a range. An example of regression is to build a correlation of the temperature of a pan of water (label) based on the time it has been heating (feature). The temperature values are continuous while the next movie is one of many discrete options.\n",
    "\n",
    "![list](https://apmonitor.com/che263/uploads/Begin_Python/list.png)\n",
    "\n",
    "Features are input values to regression or classification models. The **features are inputs** and **labels are the measured outcomes**. Below is a table of terms with terminology from machine learning, optimization, methods in `GEKKO`, and a description.\n",
    "\n",
    "| **Machine Learning** | **Optimization** | **Gekko Estimation** | **Description** |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Loss | Objective Function | `m.Minimize()` | The mathematical quantity that represents the difference between the predicted and measured outcomes |\n",
    "| Weights | Adjustable Parameters | Fixed Values (`m.FV()`) with `STATUS=1` | Adjustable values to minimize the loss function |\n",
    "| Label | Measured Outcome | Controlled Variable (`m.CV()`) with `FSTATUS=1` | Measurements of the predicted system output |\n",
    "| Feature | Measured Input | Parameter (`m.Param()`) | Input measurements that predict the outcome label |\n",
    "| Train | Optimize | Solve (`m.solve()`) | Adjust the unknown parameters (weights) to minimize the objective (loss) function |\n",
    "| Test | Evaluate | Solve with `STATUS=0` for `m.FV()` | Predict the labels with the tuned model to evaluate the performance of the classifier or regressor | \n",
    "| Regressor or Classifier | Model | `m = GEKKO()` | Mathematical equations and parameters that use feature inputs to predict an outcome label |\n",
    "\n",
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "Selection and creation of features is an important step in machine learning. Too many features may cause the classifier or regressor to increase the chances of predicting poorly. With many features, one of the inputs may be a bad value and cause a bad prediction. More features also take longer for data curation, training, and prediction. This lesson shows how to derive and select features for regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![analyze](https://apmonitor.com/che263/uploads/Begin_Python/analyze.png)\n",
    "\n",
    "### Identify Features and Label\n",
    "\n",
    "The first step in building a regressor or classifier is to determine what measurements (input features and output label) are available. You can select the data columns as features or generate [derived features](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219) with a package such as [`Featuretools`](https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96).\n",
    "\n",
    "You may want to use stock market data to give you an indicator on when to buy (`1`) or sell (`-1`). This indicator is a label. Import the daily stock data for Google for 23 days.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "url = 'http://apmonitor.com/che263/uploads/Main/goog.csv'\n",
    "data = pd.read_csv(url)\n",
    "data = data.drop(columns=['Adj Close'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features may be any of the categories that may useful in predicting a future stock price change. The `Open`, difference in `High` and `Low` (`Volitility`), difference in `Close` and `Open` (`Change`), and the `Volume` of trades are features. The `.diff()` calculates the difference and `.fillna(0)` replaces any `NaN` with zero. Add any other additional features that you would like to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Open','Volatility','Change','Volume']\n",
    "data['Volatility'] = (data['High']-data['Low']).diff()\n",
    "data['Change'] = (data['Close']-data['Open']).diff()\n",
    "# any other features?\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A label (outcome) for classification is the sign (`+` or `-`) of the close price from one day to the next. The `np.roll( ,-1)` shifts all the values up by one to indicate the change for the next day on that same row. The `np.sign()` returns the sign of the difference as an indicator of buy or sell and `.dropna()` drops the last row that is `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Close_diff'] = np.roll(data['Close'].diff(),-1)\n",
    "data=data.dropna()\n",
    "label = ['Buy/Sell']\n",
    "data['Buy/Sell'] = np.sign(data['Close_diff'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![power](https://apmonitor.com/che263/uploads/Begin_Python/power.png)\n",
    "\n",
    "### Selecting the Best Features\n",
    "\n",
    "We have now generated a number of features but we want to evaluate which ones are the best predictors of the labeled outcome. There are a variety of methods to evaluate how many features are needed (correlation) and which are the best (selection). The first thing to do is to separate into input `X` and output `y` with data scaling (`0` to `1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features+label].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "s = MinMaxScaler()\n",
    "ds = s.fit_transform(data[features+label])\n",
    "ds = pd.DataFrame(ds,columns=data[features+label].columns)\n",
    "X = ds[features]\n",
    "y = ds[label]\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "#### Selection\n",
    "\n",
    "There are statistical tests to select features that strong relationships with the output label. A tool is the scikit-learn `SelectKBest` method with associated statistical tests. This method uses a $\\chi^2$ statistical test for non-negative features to select 10 of the best features for predicting the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "scores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "scores.columns = ['Specs','Score']\n",
    "scores.index = features\n",
    "scores.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![expert](https://apmonitor.com/che263/uploads/Begin_Python/expert.png)\n",
    "\n",
    "Based on this information, drop any low scoring features with `.remove()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any low scoring features with features.remove('')\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "#### Feature Importance\n",
    "\n",
    "There is a method that comes with a Tree Based Classifier to give a score for each feature of the data. Higher score correlates to more importance and relevance for predicting the output variable. The results change each analysis due to the stochastic nature of the calculation but `Volitility` is again a factor that typically ranks highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X,np.ravel(y))\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(4).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![idea](https://apmonitor.com/che263/uploads/Begin_Python/idea.png)\n",
    "\n",
    "#### Correlation Matrix with Heatmap\n",
    "\n",
    "Correlation shows how strongly features are related to each other. A large value, either positive or negative, incidates that the values are correlated. Correlated values mean that one of them may be eliminated because they are providing similar input information. A heatmap is a symmetric visual grid of the correlation matrix. The diagonal is always 1 because each value perfectly correlates to itself. From the heatmap, determine which features are most correlated and may be able to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corrmat = ds.corr()\n",
    "top_features = corrmat.index\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(ds[top_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "b, t = plt.ylim(); plt.ylim(b+0.5, t-0.5) # addresses issue in matplotlib 3.1.1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![expert](https://apmonitor.com/che263/uploads/Begin_Python/expert.png)\n",
    "\n",
    "### TCLab Activity\n",
    "\n",
    "Consider how you could determine if the TCLab heater is on or off without knowing the `Q1` value. You would likely measure the `T1` temperature and observe if the temperature is rising or falling. However, just observing the temperature and slope is not enough because the temperature continues to rise for 10-20 seconds after the heater is turned off. As a result, you need to calculate the 2nd derivative of the temperature to classify the heater state as on or off. A positive 2nd derivative is another clue that the heater is on. \n",
    "\n",
    "![temperature](https://apmonitor.com/che263/uploads/Begin_Python/temperature.png)\n",
    "\n",
    "In the case of whether the heater is on or off (*measured outcome*), the temperature and derivatives are the *features*. Run the code below to generate temperature data with the heater `on` at 100% or `off` at 0% in 20-30 second intervals for 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tclab, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    with tclab.TCLab() as lab:\n",
    "        n = 180; t = np.linspace(0,n-1,n)        \n",
    "        Q1 = np.zeros(n); T1 = np.zeros(n)\n",
    "        Q2 = np.zeros(n); T2 = np.zeros(n)        \n",
    "        Q1[20:41] = 100.0; Q1[60:91] = 100.0\n",
    "        Q1[150:181] = 100.0; Q1[190:206] = 100.0\n",
    "        Q1[220:251] = 100.0; Q1[260:291] = 100.0\n",
    "        print('Time Q1 Q2 T1   T2')\n",
    "        for i in range(180):\n",
    "            T1[i] = lab.T1; T2[i] = lab.T2\n",
    "            lab.Q1(Q1[i])\n",
    "            if i%10==0:\n",
    "                print(int(t[i]),Q1[i],Q2[i],T1[i],T2[i])\n",
    "            time.sleep(1)\n",
    "    data = np.column_stack((t,Q1,Q2,T1,T2))\n",
    "    data7 = pd.DataFrame(data,columns=['Time','Q1','Q2','T1','T2'])\n",
    "    data7.to_csv('07-tclab.csv',index=False)\n",
    "except:\n",
    "    print('Connect TCLab to generate new data')\n",
    "    print('Importing data from online source')\n",
    "    url = 'http://apmonitor.com/do/uploads/Main/tclab_data5.txt'\n",
    "    data7=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features\n",
    "\n",
    "Create three features from the data including the temperature and the derivatives.\n",
    "\n",
    "- Temperature: $T_1$\n",
    "- Temperature derivative: $\\frac{dT_1}{dt}$\n",
    "- Temperature 2nd derivative: $\\frac{d^2T_1}{dt^2}$\n",
    "\n",
    "Add the derivatives as columns in `data7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data\n",
    "\n",
    "Scale `data7` to be between `0` and `1` with `d7 = s.fit_transform(data7)`. Don't forget to translate the scaled values back to a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Features\n",
    "\n",
    "Use `SelectKBest` to determine the best features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation\n",
    "\n",
    "Generate a Heat Map to determine the correlation of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
